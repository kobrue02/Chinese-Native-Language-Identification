{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional ML (SVM Features) for Chinese NLI\n",
    "\n",
    "Extract hand-crafted features and train classifiers (LogReg, SGD, SVM, MLP).\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload `NNP.zip` to your Google Drive under `NNP.zip`\n",
    "   - Create locally: `cd ~/Desktop/uni && zip -r NNP.zip NNP/ -x 'NNP/.venv/*' 'NNP/.git/*' 'NNP/results/*'`\n",
    "2. Run all cells (GPU not required but speeds up jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import zipfile, os, sys\n",
    "\n",
    "ZIP_PATH = '/content/drive/MyDrive/NNP.zip'\n",
    "LOCAL_DIR = '/content/NNP'\n",
    "\n",
    "if not os.path.exists(LOCAL_DIR + '/config.py'):\n",
    "    print('Extracting project to local disk...')\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
    "        zf.extractall('/content')\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('Already extracted.')\n",
    "\n",
    "os.chdir(LOCAL_DIR)\n",
    "sys.path.insert(0, LOCAL_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jieba scikit-learn tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import config\n",
    "from data_loader import load_and_split\n",
    "from features import build_features\n",
    "from models.svm import build_classifier, grid_search_svm\n",
    "from evaluate import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "train, val, test, le = load_and_split()\n",
    "label_names = list(le.classes_)\n",
    "print(f'Train: {len(train)}  Val: {len(val)}  Test: {len(test)}  Classes: {len(label_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "USE_RADICALS = True   # set to False if data/radical_map.json is missing\nUSE_DEPENDENCY = False  # requires spaCy zh_core_web_sm\n\nprint('Extracting features...')\nt0 = time.time()\nX_train_raw, X_val_raw, X_test_raw, _ = build_features(\n    train['text'].tolist(),\n    val['text'].tolist(),\n    test['text'].tolist(),\n    use_radicals=USE_RADICALS,\n    use_dependency=USE_DEPENDENCY,\n)\nprint(f'Feature extraction took {time.time() - t0:.1f}s')\nprint(f'Feature matrix shape: {X_train_raw.shape}')\n\ny_train = train['label'].values\ny_val = val['label'].values\ny_test = test['label'].values"
  },
  {
   "cell_type": "markdown",
   "source": "## Dimensionality Reduction (TruncatedSVD)\n\nThe raw TF-IDF features have ~120k dimensions. TruncatedSVD (like PCA but for sparse matrices) reduces this to a dense, lower-dimensional representation that trains much faster.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\n\nUSE_SVD = True\nSVD_COMPONENTS = 300\n\nif USE_SVD:\n    print(f'Applying TruncatedSVD: {X_train_raw.shape[1]} → {SVD_COMPONENTS} dims...')\n    t0 = time.time()\n    svd = TruncatedSVD(n_components=SVD_COMPONENTS, random_state=42)\n    scaler = StandardScaler()\n\n    X_train = scaler.fit_transform(svd.fit_transform(X_train_raw))\n    X_val = scaler.transform(svd.transform(X_val_raw))\n    X_test = scaler.transform(svd.transform(X_test_raw))\n\n    explained = svd.explained_variance_ratio_.sum()\n    print(f'Variance retained: {explained:.1%}')\n    print(f'SVD + scaling took {time.time() - t0:.1f}s')\n    print(f'Reduced shape: {X_train.shape}')\nelse:\n    X_train, X_val, X_test = X_train_raw, X_val_raw, X_test_raw\n    print(f'Skipping SVD — using raw features: {X_train.shape}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIERS = ['logreg', 'sgd', 'svm', 'mlp']\n",
    "\n",
    "rows = []\n",
    "for name in CLASSIFIERS:\n",
    "    print(f'\\n{\"=\" * 50}')\n",
    "    print(f'Training: {name}')\n",
    "    print(f'{\"=\" * 50}')\n",
    "\n",
    "    clf = build_classifier(name)\n",
    "    t0 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time.time() - t0\n",
    "    print(f'Training took {train_time:.1f}s')\n",
    "\n",
    "    val_metrics = compute_metrics(y_val, clf.predict(X_val), label_names)\n",
    "    test_metrics = compute_metrics(y_test, clf.predict(X_test), label_names)\n",
    "\n",
    "    row = {\n",
    "        'model': name,\n",
    "        'val_acc': val_metrics['accuracy'],\n",
    "        'val_f1': val_metrics['macro_f1'],\n",
    "        'val_wf1': val_metrics['weighted_f1'],\n",
    "        'test_acc': test_metrics['accuracy'],\n",
    "        'test_f1': test_metrics['macro_f1'],\n",
    "        'test_wf1': test_metrics['weighted_f1'],\n",
    "        'time_s': f'{train_time:.1f}',\n",
    "    }\n",
    "    rows.append(row)\n",
    "    print(f\"  val_acc={row['val_acc']:.4f}  val_f1={row['val_f1']:.4f}\")\n",
    "    print(f\"  test_acc={row['test_acc']:.4f}  test_f1={row['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rows)\n",
    "display(results.style.format({\n",
    "    'val_acc': '{:.4f}', 'val_f1': '{:.4f}', 'val_wf1': '{:.4f}',\n",
    "    'test_acc': '{:.4f}', 'test_f1': '{:.4f}', 'test_wf1': '{:.4f}',\n",
    "}).set_caption('Traditional ML Results (hand-crafted features)'))\n",
    "\n",
    "# Save CSV\n",
    "config.RESULTS_DIR.mkdir(exist_ok=True)\n",
    "csv_path = config.RESULTS_DIR / 'svm_results.csv'\n",
    "results.to_csv(csv_path, index=False)\n",
    "print(f'Saved {csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bar chart ─────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x = range(len(results))\n",
    "w = 0.35\n",
    "ax.bar([i - w/2 for i in x], results['test_acc'], w, label='Test Accuracy')\n",
    "ax.bar([i + w/2 for i in x], results['test_f1'], w, label='Test Macro-F1')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results['model'])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Traditional ML — Hand-crafted Features')\n",
    "ax.legend()\n",
    "for i, (acc, f1) in enumerate(zip(results['test_acc'], results['test_f1'])):\n",
    "    ax.text(i - w/2, acc + 0.01, f'{acc:.3f}', ha='center', fontsize=8)\n",
    "    ax.text(i + w/2, f1 + 0.01, f'{f1:.3f}', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LaTeX table ───────────────────────────────────────────────────────\n",
    "lines = [\n",
    "    r'\\begin{table}[htbp]',\n",
    "    r'\\centering',\n",
    "    r'\\caption{Traditional ML results (hand-crafted features).}',\n",
    "    r'\\label{tab:svm-results}',\n",
    "    r'\\begin{tabular}{lrrrr}',\n",
    "    r'\\toprule',\n",
    "    r'\\textbf{Model} & \\textbf{Val Acc} & \\textbf{Val F1} & \\textbf{Test Acc} & \\textbf{Test F1} \\\\',\n",
    "    r'\\midrule',\n",
    "]\n",
    "for _, r in results.iterrows():\n",
    "    lines.append(\n",
    "        f\"{r['model']} & {r['val_acc']:.4f} & {r['val_f1']:.4f} & \"\n",
    "        f\"{r['test_acc']:.4f} & {r['test_f1']:.4f} \\\\\\\\\"\n",
    "    )\n",
    "lines += [r'\\bottomrule', r'\\end{tabular}', r'\\end{table}']\n",
    "\n",
    "tex = '\\n'.join(lines)\n",
    "print(tex)\n",
    "\n",
    "tex_path = config.RESULTS_DIR / 'svm_results.tex'\n",
    "tex_path.write_text(tex)\n",
    "print(f'\\nSaved {tex_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy results to Drive\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "drive_results = Path('/content/drive/MyDrive/NNP_results')\n",
    "drive_results.mkdir(exist_ok=True)\n",
    "for f in config.RESULTS_DIR.iterdir():\n",
    "    shutil.copy2(f, drive_results / f.name)\n",
    "print(f'Copied results to {drive_results}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}