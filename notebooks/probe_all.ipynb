{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Probe — All Encoder Models\n",
    "\n",
    "Extract frozen embeddings from each encoder model and train an MLP classifier on top.\n",
    "Runs all models sequentially and produces a comparison table.\n",
    "\n",
    "**Instructions:**\n",
    "1. Set runtime to **GPU** (Runtime → Change runtime type → T4 GPU)\n",
    "2. Upload `JCLCv2/` folder and `index.csv` to your Google Drive under `NNP/JCLCv2/`\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_DATA_DIR = '/content/drive/MyDrive/NNP/JCLCv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers jieba scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_MODELS = [\n",
    "    'google-bert/bert-base-chinese',\n",
    "    'google-bert/bert-base-uncased',\n",
    "    'google-bert/bert-large-uncased',\n",
    "    'google-bert/bert-base-multilingual-cased',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    'voidful/albert_chinese_base',\n",
    "    'shibing624/text2vec-base-chinese',\n",
    "    'jinaai/jina-embeddings-v2-base-zh',\n",
    "    'jinaai/jina-embeddings-v3',\n",
    "    'Qwen/Qwen3-Embedding-0.6B',\n",
    "    'Qwen/Qwen3-Embedding-4B',\n",
    "    'DMetaSoul/Dmeta-embedding-zh-small',\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "DATA_DIR = Path(DRIVE_DATA_DIR)\n",
    "INDEX_CSV = DATA_DIR / 'index.csv'\n",
    "RESULTS_DIR = Path('/content/results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Smaller batch sizes for large models (adjust if OOM)\n",
    "BATCH_OVERRIDES = {\n",
    "    'google-bert/bert-large-uncased': 8,\n",
    "    'jinaai/jina-embeddings-v3': 16,\n",
    "    'Qwen/Qwen3-Embedding-4B': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(data_dir, index_csv):\n",
    "    df = pd.read_csv(\n",
    "        index_csv, header=None,\n",
    "        names=['doc_id', 'context', 'native_language', 'gender'],\n",
    "    )\n",
    "    texts = []\n",
    "    for doc_id in tqdm(df['doc_id'], desc='Loading texts'):\n",
    "        path = data_dir / f'{doc_id}.txt'\n",
    "        texts.append(path.read_text(encoding='utf-8').strip())\n",
    "    df['text'] = texts\n",
    "    return df\n",
    "\n",
    "\n",
    "def stratified_split(df, seed=42):\n",
    "    df = df.dropna(subset=['native_language'])\n",
    "    counts = df['native_language'].value_counts()\n",
    "    rare_langs = counts[counts < 3].index\n",
    "    df_rare = df[df['native_language'].isin(rare_langs)]\n",
    "    df_main = df[~df['native_language'].isin(rare_langs)]\n",
    "    df_main = df_main[df_main['native_language'].map(df_main['native_language'].value_counts()) > 1]\n",
    "\n",
    "    df_train, df_valtest = train_test_split(\n",
    "        df_main, test_size=0.2, random_state=seed,\n",
    "        stratify=df_main['native_language'],\n",
    "    )\n",
    "    df_valtest = df_valtest[df_valtest['native_language'].map(df_valtest['native_language'].value_counts()) > 1]\n",
    "    df_val, df_test = train_test_split(\n",
    "        df_valtest, test_size=0.5, random_state=seed,\n",
    "        stratify=df_valtest['native_language'],\n",
    "    )\n",
    "    df_train = pd.concat([df_train, df_rare], ignore_index=True)\n",
    "    df_train = df_train.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return df_train, df_val.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df = load_corpus(DATA_DIR, INDEX_CSV)\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['native_language'])\n",
    "train_df, val_df, test_df = stratified_split(df, RANDOM_SEED)\n",
    "label_names = list(le.classes_)\n",
    "\n",
    "train_texts = train_df['text'].tolist()\n",
    "val_texts = val_df['text'].tolist()\n",
    "test_texts = test_df['text'].tolist()\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f'Train: {len(train_df)}  Val: {len(val_df)}  Test: {len(test_df)}  Classes: {len(label_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.encodings.items()}\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_length, desc):\n",
    "    batch_size = 256\n",
    "    all_ids, all_attn, all_ttype = [], [], []\n",
    "    for i in trange(0, len(texts), batch_size, desc=desc):\n",
    "        enc = tokenizer(\n",
    "            texts[i:i+batch_size], truncation=True,\n",
    "            padding='max_length', max_length=max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        all_ids.append(enc['input_ids'])\n",
    "        all_attn.append(enc['attention_mask'])\n",
    "        if 'token_type_ids' in enc:\n",
    "            all_ttype.append(enc['token_type_ids'])\n",
    "    result = {'input_ids': torch.cat(all_ids), 'attention_mask': torch.cat(all_attn)}\n",
    "    if all_ttype:\n",
    "        result['token_type_ids'] = torch.cat(all_ttype)\n",
    "    return result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(model, encodings, batch_size, desc):\n",
    "    model.eval()\n",
    "    dataset = TextDataset(encodings)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    all_embeds = []\n",
    "    loader_iter = iter(loader)\n",
    "    for _ in trange(len(loader), desc=desc):\n",
    "        batch = next(loader_iter)\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        attn = batch['attention_mask']\n",
    "        outputs = model(**batch)\n",
    "        hidden = outputs.last_hidden_state\n",
    "        mask = attn.unsqueeze(-1).float()\n",
    "        pooled = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n",
    "        all_embeds.append(pooled.cpu().numpy())\n",
    "    return np.concatenate(all_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for i, model_name in enumerate(ENCODER_MODELS):\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'[{i+1}/{len(ENCODER_MODELS)}] {model_name}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "\n",
    "    bs = BATCH_OVERRIDES.get(model_name, BATCH_SIZE)\n",
    "    t0 = time.time()\n",
    "\n",
    "    try:\n",
    "        # Load\n",
    "        print(f'  Loading model (batch_size={bs})...')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        encoder = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(DEVICE)\n",
    "\n",
    "        # Tokenize\n",
    "        train_enc = tokenize_texts(train_texts, tokenizer, MAX_LENGTH, '  Tok train')\n",
    "        val_enc = tokenize_texts(val_texts, tokenizer, MAX_LENGTH, '  Tok val')\n",
    "        test_enc = tokenize_texts(test_texts, tokenizer, MAX_LENGTH, '  Tok test')\n",
    "\n",
    "        # Extract\n",
    "        X_train = extract_embeddings(encoder, train_enc, bs, '  Embed train')\n",
    "        X_val = extract_embeddings(encoder, val_enc, bs, '  Embed val')\n",
    "        X_test = extract_embeddings(encoder, test_enc, bs, '  Embed test')\n",
    "        emb_dim = X_train.shape[1]\n",
    "        print(f'  Embedding dim: {emb_dim}')\n",
    "\n",
    "        # Free GPU\n",
    "        del encoder, tokenizer, train_enc, val_enc, test_enc\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Train MLP\n",
    "        print('  Training MLP...')\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=(512, 256), activation='relu',\n",
    "            max_iter=200, early_stopping=True, validation_fraction=0.1,\n",
    "            random_state=RANDOM_SEED, verbose=False,\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        val_pred = clf.predict(X_val)\n",
    "        test_pred = clf.predict(X_test)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        row = {\n",
    "            'model': model_name,\n",
    "            'emb_dim': emb_dim,\n",
    "            'val_acc': accuracy_score(y_val, val_pred),\n",
    "            'val_f1': f1_score(y_val, val_pred, average='macro', zero_division=0),\n",
    "            'test_acc': accuracy_score(y_test, test_pred),\n",
    "            'test_f1': f1_score(y_test, test_pred, average='macro', zero_division=0),\n",
    "            'test_wf1': f1_score(y_test, test_pred, average='weighted', zero_division=0),\n",
    "            'time_s': f'{elapsed:.0f}',\n",
    "        }\n",
    "        rows.append(row)\n",
    "        print(f'  Done in {elapsed:.0f}s — test_acc={row[\"test_acc\"]:.4f}, test_f1={row[\"test_f1\"]:.4f}')\n",
    "\n",
    "        del clf, X_train, X_val, X_test\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f'  FAILED after {elapsed:.0f}s:')\n",
    "        traceback.print_exc()\n",
    "        rows.append({\n",
    "            'model': model_name, 'emb_dim': None,\n",
    "            'val_acc': None, 'val_f1': None,\n",
    "            'test_acc': None, 'test_f1': None, 'test_wf1': None,\n",
    "            'time_s': 'FAIL',\n",
    "        })\n",
    "        # Make sure GPU is cleared even on failure\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rows)\n",
    "display(results.style.format({\n",
    "    'val_acc': '{:.4f}', 'val_f1': '{:.4f}',\n",
    "    'test_acc': '{:.4f}', 'test_f1': '{:.4f}', 'test_wf1': '{:.4f}',\n",
    "}, na_rep='FAIL').set_caption('MLP Probe Results (frozen embeddings)'))\n",
    "\n",
    "# Save CSV\n",
    "csv_path = RESULTS_DIR / 'probe_results.csv'\n",
    "results.to_csv(csv_path, index=False)\n",
    "print(f'Saved {csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Bar chart comparison ──────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "valid = results.dropna(subset=['test_f1']).sort_values('test_f1', ascending=True)\n",
    "fig, ax = plt.subplots(figsize=(10, max(4, len(valid) * 0.5)))\n",
    "short_names = [m.split('/')[-1] for m in valid['model']]\n",
    "ax.barh(short_names, valid['test_f1'], color='steelblue')\n",
    "ax.set_xlabel('Test Macro-F1')\n",
    "ax.set_title('MLP Probe — Frozen Embeddings')\n",
    "for i, v in enumerate(valid['test_f1']):\n",
    "    ax.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LaTeX table ───────────────────────────────────────────────────────\n",
    "valid = results.dropna(subset=['test_acc'])\n",
    "lines = [\n",
    "    r'\\begin{table}[htbp]',\n",
    "    r'\\centering',\n",
    "    r'\\caption{MLP probe results (frozen embeddings).}',\n",
    "    r'\\label{tab:probe-results}',\n",
    "    r'\\resizebox{\\textwidth}{!}{%',\n",
    "    r'\\begin{tabular}{lrrrrrr}',\n",
    "    r'\\toprule',\n",
    "    r'\\textbf{Model} & \\textbf{Dim} & \\textbf{Val Acc} & \\textbf{Val F1} & \\textbf{Test Acc} & \\textbf{Test F1} & \\textbf{Time (s)} \\\\',\n",
    "    r'\\midrule',\n",
    "]\n",
    "for _, r in valid.iterrows():\n",
    "    name = r['model'].replace('_', r'\\_')\n",
    "    lines.append(\n",
    "        f\"{name} & {int(r['emb_dim'])} & {r['val_acc']:.4f} & \"\n",
    "        f\"{r['val_f1']:.4f} & {r['test_acc']:.4f} & \"\n",
    "        f\"{r['test_f1']:.4f} & {r['time_s']} \\\\\\\\\"\n",
    "    )\n",
    "lines += [r'\\bottomrule', r'\\end{tabular}}', r'\\end{table}']\n",
    "\n",
    "tex = '\\n'.join(lines)\n",
    "print(tex)\n",
    "\n",
    "tex_path = RESULTS_DIR / 'probe_results.tex'\n",
    "tex_path.write_text(tex)\n",
    "print(f'\\nSaved {tex_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy results to Drive for persistence\n",
    "drive_results = Path(DRIVE_DATA_DIR).parent / 'results'\n",
    "drive_results.mkdir(exist_ok=True)\n",
    "import shutil\n",
    "for f in RESULTS_DIR.iterdir():\n",
    "    shutil.copy2(f, drive_results / f.name)\n",
    "print(f'Copied results to {drive_results}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
